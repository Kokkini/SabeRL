# Data Model: RL Agent Training

**Feature**: RL Agent Training  
**Date**: 2025-01-27  
**Purpose**: Define data structures and relationships for the RL training system

## Core Entities

### NeuralNetwork
Represents the AI brain that processes game state and outputs movement decisions.

**Attributes**:
- `id`: string - Unique identifier
- `architecture`: object - Network structure (layers, activations)
- `weights`: tf.Tensor[] - Model parameters
- `optimizer`: object - Training optimizer configuration
- `createdAt`: Date - Creation timestamp
- `lastTrained`: Date - Last training timestamp

**Relationships**:
- Owned by: TrainingSession
- Used by: PolicyAgent, ValueAgent

**State Transitions**:
- `initialized` → `trained` → `saved` → `loaded`

### TrainingSession
Represents an active learning period with metrics and performance tracking.

**Attributes**:
- `id`: string - Unique session identifier
- `startTime`: Date - Session start timestamp
- `endTime`: Date - Session end timestamp (null if active)
- `status`: string - 'active', 'paused', 'completed', 'stopped'
- `totalGames`: number - Total games played
- `currentModel`: NeuralNetwork - Active neural network
- `metrics`: TrainingMetrics - Performance data
- `config`: object - Training parameters

**Relationships**:
- Contains: NeuralNetwork, TrainingMetrics
- Manages: ParallelGame[]

**State Transitions**:
- `created` → `active` → `paused` → `active` → `completed`
- `active` → `stopped` (user cancellation)

### GameState
Represents the current perception data provided to the AI.

**Attributes**:
- `playerPosition`: tf.Tensor - Player coordinates [x, y]
- `opponentPosition`: tf.Tensor - AI opponent coordinates [x, y]
- `playerSaberAngle`: number - Player saber angle in radians
- `playerSaberAngularVelocity`: number - Player saber rotation speed
- `opponentSaberAngle`: number - Opponent saber angle in radians
- `opponentSaberAngularVelocity`: number - Opponent saber rotation speed
- `timestamp`: number - Game time in milliseconds

**Relationships**:
- Input to: PolicyAgent
- Generated by: TrainingGame

### MovementDecision
Represents the AI's chosen action for the next frame interval.

**Attributes**:
- `action`: string - WASD key ('W', 'A', 'S', 'D')
- `confidence`: number - Decision confidence (0-1)
- `timestamp`: number - Decision time
- `frameInterval`: number - Frames to hold action

**Relationships**:
- Output from: PolicyAgent
- Applied to: Player entity

### TrainingMetrics
Represents performance data collected during training.

**Attributes**:
- `winRate`: number - Percentage of games won (0-1)
- `averageGameLength`: number - Average game duration in seconds
- `rewardStats`: object - {avg, min, max, std} of last 100 games
- `trainingTime`: number - Total training time in seconds
- `gamesCompleted`: number - Number of games finished
- `learningRate`: number - Current learning rate
- `explorationRate`: number - Current exploration rate

**Relationships**:
- Part of: TrainingSession
- Updated by: MetricsTracker

### ParallelGame
Represents a headless game instance for parallel training.

**Attributes**:
- `id`: string - Unique game identifier
- `gameState`: GameState - Current game state
- `playerAgent`: PolicyAgent - AI agent controlling player
- `opponentAgent`: PolicyAgent - AI agent controlling opponent
- `isActive`: boolean - Whether game is running
- `startTime`: Date - Game start timestamp
- `endTime`: Date - Game end timestamp (null if active)

**Relationships**:
- Managed by: TrainingSession
- Uses: PolicyAgent, GameState

## Value Objects

### RewardCalculation
Represents the reward computation for a game outcome.

**Attributes**:
- `baseReward`: number - Win/loss reward (+1/-1)
- `timePenalty`: number - Time-based penalty
- `totalReward`: number - Final reward value
- `gameLength`: number - Game duration in seconds

### ModelCheckpoint
Represents a saved state of the neural network.

**Attributes**:
- `modelId`: string - Reference to NeuralNetwork
- `weights`: object - Serialized model weights
- `metadata`: object - Training progress, version info
- `savedAt`: Date - Checkpoint timestamp

## Data Flow

### Training Initialization
1. Create TrainingSession with default config
2. Initialize NeuralNetwork with random weights
3. Create PolicyAgent with neural network
4. Set up ParallelGame instances

### Game Execution Loop
1. TrainingGame generates GameState
2. PolicyAgent processes GameState → MovementDecision
3. Game applies MovementDecision to Player
4. Game updates and checks for win/loss
5. RewardCalculator computes reward
6. ExperienceBuffer stores (state, action, reward, next_state)

### Training Update
1. ExperienceBuffer provides batch of experiences
2. PPOTrainer/A2CTrainer updates NeuralNetwork weights
3. MetricsTracker updates TrainingMetrics
4. ModelManager saves checkpoint if needed

### Model Persistence
1. NeuralNetwork weights serialized to JSON
2. Stored in localStorage with unique key
3. Metadata includes architecture and training info
4. Can be loaded to resume training

## Validation Rules

### NeuralNetwork
- Weights must be valid tensors
- Architecture must match input/output dimensions
- Optimizer must be compatible with network

### TrainingSession
- Start time must be before end time
- Status must be valid enum value
- Total games must be non-negative

### GameState
- All positions must be within arena bounds
- Angles must be normalized to [0, 2π]
- Timestamp must be positive

### MovementDecision
- Action must be valid WASD key
- Confidence must be in [0, 1] range
- Frame interval must be positive

## Storage Strategy

### localStorage (Small Data)
- NeuralNetwork weights (JSON serialized)
- TrainingSession metadata
- User preferences and config

### IndexedDB (Large Data)
- ExperienceBuffer data
- TrainingMetrics history
- Model checkpoints

### Memory (Runtime Data)
- Active game states
- Current training metrics
- UI state and visualization data
